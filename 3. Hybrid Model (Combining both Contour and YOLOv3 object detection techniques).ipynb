{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broke-custody",
   "metadata": {},
   "source": [
    "# Combination of Contour and YOLOv3\n",
    "This notebook shows the optimization of result by combining both the techniques. Please refer to the seperate implementaion of Contour method and YOLOv3 before going through this notebook.\n",
    "\n",
    "As seen previously, contour method gives an overall accuracy of 60.24% whereas YOLOv3 gave 74.10%. Although YOLOv3 gives better result, we'll try to optimize the result even more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-spectrum",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "convertible-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Flatten, MaxPooling2D, Dropout, Conv2D\n",
    "from IPython.display import Image\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-helping",
   "metadata": {},
   "source": [
    "## Importing the dataset of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "homeless-magnet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>NUMBER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>KL55R2473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GJW115A1138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>KL16J3636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MH20EE7598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>PJJ1L76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>182</td>\n",
       "      <td>DHF5B15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>183</td>\n",
       "      <td>DBJ5R59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>184</td>\n",
       "      <td>LVY8X45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>185</td>\n",
       "      <td>YYD6B61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>186</td>\n",
       "      <td>CHI4A30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       NUMBER\n",
       "0      1    KL55R2473\n",
       "1      2  GJW115A1138\n",
       "2      3    KL16J3636\n",
       "3      4   MH20EE7598\n",
       "4      5      PJJ1L76\n",
       "..   ...          ...\n",
       "181  182      DHF5B15\n",
       "182  183      DBJ5R59\n",
       "183  184      LVY8X45\n",
       "184  185      YYD6B61\n",
       "185  186      CHI4A30\n",
       "\n",
       "[186 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.read_excel('test_dataset/labels.xlsx')\n",
    "labels['ID']=labels['ID'].map(str)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-customs",
   "metadata": {},
   "source": [
    "## Functions used in the Contour Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greater-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x1, x2, y1, y2):\n",
    "    return ((x1-x2)**2+(y1-y2)**2)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-investigation",
   "metadata": {},
   "source": [
    "## Functions used by YOLOv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "noble-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load names of classes\n",
    "classesFile = \"yolo_utils/classes.names\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proper-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the configuration and weight files for the model and load the network using them.\n",
    "modelConfiguration = \"yolo_utils/darknet-yolov3.cfg\";\n",
    "modelWeights = \"yolo_utils/lapi.weights\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fleet-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "confThreshold = 0.5  #Confidence threshold\n",
    "nmsThreshold = 0.4  #Non-maximum suppression threshold\n",
    "\n",
    "inpWidth = 416     #Width of network's input image\n",
    "inpHeight = 416     #Height of network's input image\n",
    "\n",
    "classes = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sophisticated-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the output layers\n",
    "def getOutputsNames(net):\n",
    "    # Get the names of all the layers in the network\n",
    "    layersNames = net.getLayerNames()\n",
    "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
    "    return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sharp-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the bounding boxes with low confidence using non-maxima suppression\n",
    "def postprocess(frame, outs):\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "    \n",
    "    # Scan through all the bounding boxes output from the network and keep only the\n",
    "    # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
    "    classIds = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            #if detection[4]>0.001:\n",
    "            scores = detection[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            #if scores[classId]>confThreshold:\n",
    "            confidence = scores[classId]\n",
    "            if confidence > confThreshold:\n",
    "                center_x = int(detection[0] * frameWidth)\n",
    "                center_y = int(detection[1] * frameHeight)\n",
    "                width = int(detection[2] * frameWidth)\n",
    "                height = int(detection[3] * frameHeight)\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                classIds.append(classId)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "    # lower confidences.\n",
    "    cropped=None\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        \n",
    "        # calculate bottom and right\n",
    "        bottom = top + height\n",
    "        right = left + width\n",
    "        \n",
    "        #crop the plate out\n",
    "        cropped = frame[top:bottom, left:right].copy()\n",
    "    if cropped is not None:\n",
    "        return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "classical-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the predicted bounding box\n",
    "def drawPred(classId, conf, left, top, right, bottom, frame):\n",
    "    # Draw a bounding box.\n",
    "    cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 3)\n",
    "\n",
    "    label = '%.2f' % conf\n",
    "\n",
    "    # Get the label for the class name and its confidence\n",
    "    if classes:\n",
    "        assert(classId < len(classes))\n",
    "        label = '%s:%s' % (classes[classId], label)\n",
    "\n",
    "    #Display the label at the top of the bounding box\n",
    "    labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    top = max(top, labelSize[1])\n",
    "    cv2.rectangle(frame, (left, top - round(1.5*labelSize[1])), (left + round(1.5*labelSize[0]), top + baseLine), (0, 0, 255), cv2.FILLED)\n",
    "    cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-laundry",
   "metadata": {},
   "source": [
    "## Functions used in Character Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "champion-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match contours to license plate or character template\n",
    "def find_contours(dimensions, img) :\n",
    "\n",
    "    # Find all contours in the image\n",
    "    cntrs, _ = cv2.findContours(img.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Retrieve potential dimensions\n",
    "    lower_width = dimensions[0]\n",
    "    upper_width = dimensions[1]\n",
    "    lower_height = dimensions[2]\n",
    "    upper_height = dimensions[3]\n",
    "    \n",
    "    # Check largest 5 or  15 contours for license plate or character respectively\n",
    "    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]\n",
    "    \n",
    "    ii = cv2.imread('contour.jpg')\n",
    "    \n",
    "    x_cntr_list = []\n",
    "    target_contours = []\n",
    "    img_res = []\n",
    "    for cntr in cntrs :\n",
    "        # detects contour in binary image and returns the coordinates of rectangle enclosing it\n",
    "        intX, intY, intWidth, intHeight = cv2.boundingRect(cntr)\n",
    "        \n",
    "        # checking the dimensions of the contour to filter out the characters by contour's size\n",
    "        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height :\n",
    "            x_cntr_list.append(intX) #stores the x coordinate of the character's contour, to used later for indexing the contours\n",
    "\n",
    "            char_copy = np.zeros((44,24))\n",
    "            # extracting each character using the enclosing rectangle's coordinates.\n",
    "            char = img[intY:intY+intHeight, intX:intX+intWidth]\n",
    "            char = cv2.resize(char, (20, 40))\n",
    "            \n",
    "            cv2.rectangle(ii, (intX,intY), (intWidth+intX, intY+intHeight), (50,21,200), 2)\n",
    "\n",
    "            # Make result formatted for classification: invert colors\n",
    "            char = cv2.subtract(255, char)\n",
    "\n",
    "            # Resize the image to 24x44 with black border\n",
    "            char_copy[2:42, 2:22] = char\n",
    "            char_copy[0:2, :] = 0\n",
    "            char_copy[:, 0:2] = 0\n",
    "            char_copy[42:44, :] = 0\n",
    "            char_copy[:, 22:24] = 0\n",
    "\n",
    "            img_res.append(char_copy) # List that stores the character's binary image (unsorted)\n",
    "\n",
    "    # arbitrary function that stores sorted list of character indeces\n",
    "    indices = sorted(range(len(x_cntr_list)), key=lambda k: x_cntr_list[k])\n",
    "    img_res_copy = []\n",
    "    for idx in indices:\n",
    "        img_res_copy.append(img_res[idx])# stores character images according to their index\n",
    "    img_res = np.array(img_res_copy)\n",
    "\n",
    "    return img_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "western-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find characters in the resulting images\n",
    "def segment_characters(image) :\n",
    "\n",
    "    # Preprocess cropped license plate image\n",
    "    img_lp = cv2.resize(image, (333, 75))\n",
    "    img_gray_lp = cv2.cvtColor(img_lp, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_binary_lp = cv2.threshold(img_gray_lp, 200, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    img_binary_lp = cv2.erode(img_binary_lp, (3,3))\n",
    "    img_binary_lp = cv2.dilate(img_binary_lp, (3,3))\n",
    "\n",
    "    LP_WIDTH = img_binary_lp.shape[0]\n",
    "    LP_HEIGHT = img_binary_lp.shape[1]\n",
    "\n",
    "    # Make borders white\n",
    "    img_binary_lp[0:3,:] = 255\n",
    "    img_binary_lp[:,0:3] = 255\n",
    "    img_binary_lp[72:75,:] = 255\n",
    "    img_binary_lp[:,330:333] = 255\n",
    "\n",
    "    # Estimations of character contours sizes of cropped license plates\n",
    "    dimensions = [LP_WIDTH/6,\n",
    "                       LP_WIDTH/2,\n",
    "                       LP_HEIGHT/10,\n",
    "                       2*LP_HEIGHT/3]\n",
    "    cv2.imwrite('contour.jpg',img_binary_lp)\n",
    "\n",
    "    # Get contours within cropped license plate\n",
    "    char_list = find_contours(dimensions, img_binary_lp)\n",
    "\n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-formation",
   "metadata": {},
   "source": [
    "## Loading the weights of CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "manufactured-montgomery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1c244530448>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new model instance\n",
    "loaded_model = Sequential()\n",
    "loaded_model.add(Conv2D(16, (22,22), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "loaded_model.add(Conv2D(32, (16,16), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "loaded_model.add(Conv2D(64, (8,8), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "loaded_model.add(Conv2D(64, (4,4), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "loaded_model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "loaded_model.add(Dropout(0.4))\n",
    "loaded_model.add(Flatten())\n",
    "loaded_model.add(Dense(128, activation='relu'))\n",
    "loaded_model.add(Dense(36, activation='softmax'))\n",
    "\n",
    "# Restore the weights\n",
    "loaded_model.load_weights('checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "precise-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the output\n",
    "def fix_dimension(img): \n",
    "  new_img = np.zeros((28,28,3))\n",
    "  for i in range(3):\n",
    "    new_img[:,:,i] = img\n",
    "  return new_img\n",
    "  \n",
    "def show_results(count):\n",
    "    dic = {}\n",
    "    characters = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    for i,c in enumerate(characters):\n",
    "        dic[i] = c\n",
    "\n",
    "    output = []\n",
    "    for i,ch in enumerate(char): #iterating over the characters\n",
    "        img_ = cv2.resize(ch, (28,28), interpolation=cv2.INTER_AREA)\n",
    "        img = fix_dimension(img_)\n",
    "        img = img.reshape(1,28,28,3) #preparing image for the model\n",
    "        y_ = loaded_model.predict_classes(img)[0] #predicting the class\n",
    "        character = dic[y_] #\n",
    "        output.append(character) #storing the result in a list\n",
    "        \n",
    "    plate_number = ''.join(output)\n",
    "    if plate_number==row:\n",
    "        count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-flexibility",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "overall-divorce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.96385542168674%\n"
     ]
    }
   ],
   "source": [
    "file_list=os.listdir(r\"test_dataset/images\")\n",
    "count=0\n",
    "for path in file_list:\n",
    "#for entry in labels['ID']:\n",
    "    input_path = 'test_dataset/images/'+path\n",
    "    is_video = False\n",
    "    no=path[:-4]\n",
    "    row=labels['NUMBER'].where(labels['ID'] == no).dropna().values[0]\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    outputFile = input_path + '_yolo_out_py.jpg'\n",
    "\n",
    "    while cv2.waitKey(1) < 0:\n",
    "\n",
    "        # get frame from the video\n",
    "        hasFrame, frame = cap.read() #frame: an image object from cv2\n",
    "\n",
    "        # Stop the program if reached end of video\n",
    "        if not hasFrame:\n",
    "            break\n",
    "\n",
    "        # Create a 4D blob from a frame.\n",
    "        try:\n",
    "            blob = cv2.dnn.blobFromImage(frame, 1/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        # Sets the input to the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Runs the forward pass to get output of the output layers\n",
    "        outs = net.forward(getOutputsNames(net))\n",
    "\n",
    "        # Remove the bounding boxes with low confidence\n",
    "        cropped = postprocess(frame, outs)\n",
    "        if cropped is not None:\n",
    "            # Put efficiency information. The function getPerfProfile returns the overall time for inference(t) and the timings for each of the layers(in layersTimes)\n",
    "            t, _ = net.getPerfProfile()\n",
    "            label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
    "            #cv.putText(frame, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "            # Write the frame with the detection boxes\n",
    "            if is_video:\n",
    "                vid_writer.write(frame.astype(np.uint8))\n",
    "            else:\n",
    "                #plt.imshow(cropped)\n",
    "                #plt.show()\n",
    "                char=segment_characters(cropped)\n",
    "                count=show_results(count)\n",
    "        else:\n",
    "            ####\n",
    "            image = cv2.imread('test_dataset/images/'+path)\n",
    "            # Resize the image - change width to 500\n",
    "            image = imutils.resize(image, width=500)\n",
    "            img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # RGB to Gray scale conversion\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Noise removal with iterative bilateral filter(removes noise while preserving edges)\n",
    "            gray = cv2.bilateralFilter(gray, 11, 17, 17)\n",
    "\n",
    "            # Find Edges of the grayscale image\n",
    "            edged = cv2.Canny(gray, 170, 200)\n",
    "\n",
    "            # Find contours based on Edges\n",
    "            cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "            cnts=sorted(cnts, key = cv2.contourArea, reverse = True)[:30] #sort contours based on their area keeping minimum required area as '30' (anything smaller than this will not be considered)\n",
    "            NumberPlateCnt = None #we currently have no Number plate contour\n",
    "\n",
    "            # loop over our contours to find the best possible approximate contour of number plate\n",
    "            for c in cnts:\n",
    "                    peri = cv2.arcLength(c, True)\n",
    "                    approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "                    if len(approx) == 4:  # Select the contour with 4 corners\n",
    "                        NumberPlateCnt = approx #This is our approx Number Plate Contour\n",
    "                        x,y,w,h = cv2.boundingRect(c)\n",
    "                        ROI = img[y:y+h, x:x+w]\n",
    "                        break\n",
    "\n",
    "            idx=0\n",
    "            m=0\n",
    "            if NumberPlateCnt is None:\n",
    "                continue\n",
    "            for i in range(4):\n",
    "                if NumberPlateCnt[i][0][1]>m:\n",
    "                    idx=i\n",
    "                    m=NumberPlateCnt[i][0][1]\n",
    "            if idx==0:\n",
    "                pin=3\n",
    "            else:\n",
    "                pin=idx-1\n",
    "            if idx==3:\n",
    "                nin=0\n",
    "            else:\n",
    "                nin=idx+1\n",
    "\n",
    "            p=dist(NumberPlateCnt[idx][0][0], NumberPlateCnt[pin][0][0], NumberPlateCnt[idx][0][1], NumberPlateCnt[pin][0][1])\n",
    "            n=dist(NumberPlateCnt[idx][0][0], NumberPlateCnt[nin][0][0], NumberPlateCnt[idx][0][1], NumberPlateCnt[nin][0][1])\n",
    "\n",
    "            if p>n:\n",
    "                if NumberPlateCnt[pin][0][0]<NumberPlateCnt[idx][0][0]:\n",
    "                    left=pin\n",
    "                    right=idx\n",
    "                else:\n",
    "                    left=idx\n",
    "                    right=pin\n",
    "                d=p\n",
    "            else:\n",
    "                if NumberPlateCnt[nin][0][0]<NumberPlateCnt[idx][0][0]:\n",
    "                    left=nin\n",
    "                    right=idx\n",
    "                else:\n",
    "                    left=idx\n",
    "                    right=nin\n",
    "                d=n\n",
    "            left_x=NumberPlateCnt[left][0][0]\n",
    "            left_y=NumberPlateCnt[left][0][1]\n",
    "            right_x=NumberPlateCnt[right][0][0]\n",
    "            right_y=NumberPlateCnt[right][0][1]\n",
    "\n",
    "            opp=right_y-left_y\n",
    "            hyp=((left_x-right_x)**2+(left_y-right_y)**2)**0.5\n",
    "            sin=opp/hyp\n",
    "            theta=math.asin(sin)*57.2958\n",
    "\n",
    "            image_center = tuple(np.array(ROI.shape[1::-1]) / 2)\n",
    "            rot_mat = cv2.getRotationMatrix2D(image_center, theta, 1.0)\n",
    "            result = cv2.warpAffine(ROI, rot_mat, ROI.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "\n",
    "            if opp>0:\n",
    "                h=result.shape[0]-opp//2\n",
    "            else:\n",
    "                h=result.shape[0]+opp//2\n",
    "\n",
    "            result=result[0:h, :]\n",
    "            char=segment_characters(result)\n",
    "            count=show_results(count)\n",
    "\n",
    "print(\"Accuracy: \"+str((count/166)*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-insight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
